{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3f44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ultralytics.yolo in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (0.0.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ultralytics.yolo) (1.24.3)\n",
      "Requirement already satisfied: build in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics.yolo) (1.2.1)\n",
      "Requirement already satisfied: twine in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics.yolo) (5.0.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from build->ultralytics.yolo) (23.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from build->ultralytics.yolo) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from build->ultralytics.yolo) (0.4.6)\n",
      "Requirement already satisfied: pkginfo>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from twine->ultralytics.yolo) (1.9.6)\n",
      "Requirement already satisfied: readme-renderer>=35.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from twine->ultralytics.yolo) (43.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from twine->ultralytics.yolo) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from twine->ultralytics.yolo) (1.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from twine->ultralytics.yolo) (1.26.16)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from twine->ultralytics.yolo) (6.0.0)\n",
      "Requirement already satisfied: keyring>=15.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from twine->ultralytics.yolo) (23.13.1)\n",
      "Requirement already satisfied: rfc3986>=1.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from twine->ultralytics.yolo) (2.0.0)\n",
      "Requirement already satisfied: rich>=12.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from twine->ultralytics.yolo) (13.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=3.6->twine->ultralytics.yolo) (3.11.0)\n",
      "Requirement already satisfied: jaraco.classes in c:\\programdata\\anaconda3\\lib\\site-packages (from keyring>=15.1->twine->ultralytics.yolo) (3.2.1)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keyring>=15.1->twine->ultralytics.yolo) (0.2.0)\n",
      "Requirement already satisfied: nh3>=0.2.14 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from readme-renderer>=35.0->twine->ultralytics.yolo) (0.2.17)\n",
      "Requirement already satisfied: docutils>=0.13.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from readme-renderer>=35.0->twine->ultralytics.yolo) (0.18.1)\n",
      "Requirement already satisfied: Pygments>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from readme-renderer>=35.0->twine->ultralytics.yolo) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->ultralytics.yolo) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->ultralytics.yolo) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->ultralytics.yolo) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=12.0.0->twine->ultralytics.yolo) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine->ultralytics.yolo) (0.1.0)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from jaraco.classes->keyring>=15.1->twine->ultralytics.yolo) (8.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics.yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2e6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hydra\n",
      "  Using cached Hydra-2.5.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: hydra\n",
      "  Building wheel for hydra (setup.py): started\n",
      "  Building wheel for hydra (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for hydra\n",
      "Failed to build hydra\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [9 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  copying src\\hydra.py -> build\\lib.win-amd64-cpython-311\n",
      "  running build_ext\n",
      "  building '_hydra' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hydra\n",
      "ERROR: Could not build wheels for hydra, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c05c4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics.yolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcudnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcudnn\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasePredictor\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_CONFIG, ROOT, ops\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_imgsz\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.yolo'"
     ]
    }
   ],
   "source": [
    "# import hydra\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "from ultralytics.yolo.engine.predictor import BasePredictor\n",
    "from ultralytics.yolo.utils import DEFAULT_CONFIG, ROOT, ops\n",
    "from ultralytics.yolo.utils.checks import check_imgsz\n",
    "from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box\n",
    "\n",
    "import cv2\n",
    "from deep_sort_pytorch.utils.parser import get_config\n",
    "from deep_sort_pytorch.deep_sort import DeepSort\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "data_deque = {}\n",
    "\n",
    "deepsort = None\n",
    "\n",
    "object_counter = {}\n",
    "\n",
    "object_counter1 = {}\n",
    "\n",
    "line = [(100, 500), (1050, 500)]\n",
    "speed_line_queue = {}\n",
    "def estimatespeed(Location1, Location2):\n",
    "    #Euclidean Distance Formula\n",
    "    d_pixel = math.sqrt(math.pow(Location2[0] - Location1[0], 2) + math.pow(Location2[1] - Location1[1], 2))\n",
    "    # defining thr pixels per meter\n",
    "    ppm = 8\n",
    "    d_meters = d_pixel/ppm\n",
    "    time_constant = 15*3.6\n",
    "    #distance = speed/time\n",
    "    speed = d_meters * time_constant\n",
    "\n",
    "    return int(speed)\n",
    "def init_tracker():\n",
    "    global deepsort\n",
    "    cfg_deep = get_config()\n",
    "    cfg_deep.merge_from_file(\"deep_sort_pytorch/configs/deep_sort.yaml\")\n",
    "\n",
    "    deepsort= DeepSort(cfg_deep.DEEPSORT.REID_CKPT,\n",
    "                            max_dist=cfg_deep.DEEPSORT.MAX_DIST, min_confidence=cfg_deep.DEEPSORT.MIN_CONFIDENCE,\n",
    "                            nms_max_overlap=cfg_deep.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg_deep.DEEPSORT.MAX_IOU_DISTANCE,\n",
    "                            max_age=cfg_deep.DEEPSORT.MAX_AGE, n_init=cfg_deep.DEEPSORT.N_INIT, nn_budget=cfg_deep.DEEPSORT.NN_BUDGET,\n",
    "                            use_cuda=True)\n",
    "##########################################################################################\n",
    "def xyxy_to_xywh(*xyxy):\n",
    "    \"\"\"\" Calculates the relative bounding box from absolute pixel values. \"\"\"\n",
    "    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n",
    "    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n",
    "    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n",
    "    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n",
    "    x_c = (bbox_left + bbox_w / 2)\n",
    "    y_c = (bbox_top + bbox_h / 2)\n",
    "    w = bbox_w\n",
    "    h = bbox_h\n",
    "    return x_c, y_c, w, h\n",
    "\n",
    "def xyxy_to_tlwh(bbox_xyxy):\n",
    "    tlwh_bboxs = []\n",
    "    for i, box in enumerate(bbox_xyxy):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        top = x1\n",
    "        left = y1\n",
    "        w = int(x2 - x1)\n",
    "        h = int(y2 - y1)\n",
    "        tlwh_obj = [top, left, w, h]\n",
    "        tlwh_bboxs.append(tlwh_obj)\n",
    "    return tlwh_bboxs\n",
    "\n",
    "def compute_color_for_labels(label):\n",
    "    \"\"\"\n",
    "    Simple function that adds fixed color depending on the class\n",
    "    \"\"\"\n",
    "    if label == 0: #person\n",
    "        color = (85,45,255)\n",
    "    elif label == 2: # Car\n",
    "        color = (222,82,175)\n",
    "    elif label == 3:  # Motobike\n",
    "        color = (0, 204, 255)\n",
    "    elif label == 5:  # Bus\n",
    "        color = (0, 149, 255)\n",
    "    else:\n",
    "        color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)\n",
    "\n",
    "def draw_border(img, pt1, pt2, color, thickness, r, d):\n",
    "    x1,y1 = pt1\n",
    "    x2,y2 = pt2\n",
    "    # Top left\n",
    "    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)\n",
    "    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)\n",
    "    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\n",
    "    # Top right\n",
    "    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)\n",
    "    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)\n",
    "    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\n",
    "    # Bottom left\n",
    "    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)\n",
    "    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)\n",
    "    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\n",
    "    # Bottom right\n",
    "    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)\n",
    "    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)\n",
    "    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)\n",
    "\n",
    "    cv2.rectangle(img, (x1 + r, y1), (x2 - r, y2), color, -1, cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x1, y1 + r), (x2, y2 - r - d), color, -1, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.circle(img, (x1 +r, y1+r), 2, color, 12)\n",
    "    cv2.circle(img, (x2 -r, y1+r), 2, color, 12)\n",
    "    cv2.circle(img, (x1 +r, y2-r), 2, color, 12)\n",
    "    cv2.circle(img, (x2 -r, y2-r), 2, color, 12)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def UI_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "\n",
    "        img = draw_border(img, (c1[0], c1[1] - t_size[1] -3), (c1[0] + t_size[0], c1[1]+3), color, 1, 8, 2)\n",
    "\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def intersect(A,B,C,D):\n",
    "    return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n",
    "\n",
    "def ccw(A,B,C):\n",
    "    return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])\n",
    "\n",
    "\n",
    "def get_direction(point1, point2):\n",
    "    direction_str = \"\"\n",
    "\n",
    "    # calculate y axis direction\n",
    "    if point1[1] > point2[1]:\n",
    "        direction_str += \"South\"\n",
    "    elif point1[1] < point2[1]:\n",
    "        direction_str += \"North\"\n",
    "    else:\n",
    "        direction_str += \"\"\n",
    "\n",
    "    # calculate x axis direction\n",
    "    if point1[0] > point2[0]:\n",
    "        direction_str += \"East\"\n",
    "    elif point1[0] < point2[0]:\n",
    "        direction_str += \"West\"\n",
    "    else:\n",
    "        direction_str += \"\"\n",
    "\n",
    "    return direction_str\n",
    "def draw_boxes(img, bbox, names,object_id, identities=None, offset=(0, 0)):\n",
    "    cv2.line(img, line[0], line[1], (46,162,112), 3)\n",
    "\n",
    "    height, width, _ = img.shape\n",
    "    # remove tracked point from buffer if object is lost\n",
    "    for key in list(data_deque):\n",
    "      if key not in identities:\n",
    "        data_deque.pop(key)\n",
    "\n",
    "    for i, box in enumerate(bbox):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        x1 += offset[0]\n",
    "        x2 += offset[0]\n",
    "        y1 += offset[1]\n",
    "        y2 += offset[1]\n",
    "\n",
    "        # code to find center of bottom edge\n",
    "        center = (int((x2+x1)/ 2), int((y2+y2)/2))\n",
    "\n",
    "        # get ID of object\n",
    "        id = int(identities[i]) if identities is not None else 0\n",
    "\n",
    "        # create new buffer for new object\n",
    "        if id not in data_deque:  \n",
    "          data_deque[id] = deque(maxlen= 64)\n",
    "          speed_line_queue[id] = []\n",
    "        color = compute_color_for_labels(object_id[i])\n",
    "        obj_name = names[object_id[i]]\n",
    "        label = '{}{:d}'.format(\"\", id) + \":\"+ '%s' % (obj_name)\n",
    "\n",
    "        # add center to buffer\n",
    "        data_deque[id].appendleft(center)\n",
    "        if len(data_deque[id]) >= 2:\n",
    "          direction = get_direction(data_deque[id][0], data_deque[id][1])\n",
    "          object_speed = estimatespeed(data_deque[id][1], data_deque[id][0])\n",
    "          speed_line_queue[id].append(object_speed)\n",
    "          if intersect(data_deque[id][0], data_deque[id][1], line[0], line[1]):\n",
    "              cv2.line(img, line[0], line[1], (255, 255, 255), 3)\n",
    "              if \"South\" in direction:\n",
    "                if obj_name not in object_counter:\n",
    "                    object_counter[obj_name] = 1\n",
    "                else:\n",
    "                    object_counter[obj_name] += 1\n",
    "              if \"North\" in direction:\n",
    "                if obj_name not in object_counter1:\n",
    "                    object_counter1[obj_name] = 1\n",
    "                else:\n",
    "                    object_counter1[obj_name] += 1\n",
    "\n",
    "        try:\n",
    "            label = label + \" \" + str(sum(speed_line_queue[id])//len(speed_line_queue[id])) + \"km/h\"\n",
    "        except:\n",
    "            pass\n",
    "        UI_box(box, img, label=label, color=color, line_thickness=2)\n",
    "        # draw trail\n",
    "        for i in range(1, len(data_deque[id])):\n",
    "            # check if on buffer value is none\n",
    "            if data_deque[id][i - 1] is None or data_deque[id][i] is None:\n",
    "                continue\n",
    "            # generate dynamic thickness of trails\n",
    "            thickness = int(np.sqrt(64 / float(i + i)) * 1.5)\n",
    "            # draw trails\n",
    "            cv2.line(img, data_deque[id][i - 1], data_deque[id][i], color, thickness)\n",
    "    \n",
    "    #4. Display Count in top right corner\n",
    "        for idx, (key, value) in enumerate(object_counter1.items()):\n",
    "            cnt_str = str(key) + \":\" +str(value)\n",
    "            cv2.line(img, (width - 500,25), (width,25), [85,45,255], 40)\n",
    "            cv2.putText(img, f'Number of Vehicles Entering', (width - 500, 35), 0, 1, [225, 255, 255], thickness=2, lineType=cv2.LINE_AA)\n",
    "            cv2.line(img, (width - 150, 65 + (idx*40)), (width, 65 + (idx*40)), [85, 45, 255], 30)\n",
    "            cv2.putText(img, cnt_str, (width - 150, 75 + (idx*40)), 0, 1, [255, 255, 255], thickness = 2, lineType = cv2.LINE_AA)\n",
    "\n",
    "        for idx, (key, value) in enumerate(object_counter.items()):\n",
    "            cnt_str1 = str(key) + \":\" +str(value)\n",
    "            cv2.line(img, (20,25), (500,25), [85,45,255], 40)\n",
    "            cv2.putText(img, f'Numbers of Vehicles Leaving', (11, 35), 0, 1, [225, 255, 255], thickness=2, lineType=cv2.LINE_AA)    \n",
    "            cv2.line(img, (20,65+ (idx*40)), (127,65+ (idx*40)), [85,45,255], 30)\n",
    "            cv2.putText(img, cnt_str1, (11, 75+ (idx*40)), 0, 1, [225, 255, 255], thickness=2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "class DetectionPredictor(BasePredictor):\n",
    "\n",
    "    def get_annotator(self, img):\n",
    "        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        img = torch.from_numpy(img).to(self.model.device)\n",
    "        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\n",
    "        img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        return img\n",
    "\n",
    "    def postprocess(self, preds, img, orig_img):\n",
    "        preds = ops.non_max_suppression(preds,\n",
    "                                        self.args.conf,\n",
    "                                        self.args.iou,\n",
    "                                        agnostic=self.args.agnostic_nms,\n",
    "                                        max_det=self.args.max_det)\n",
    "\n",
    "        for i, pred in enumerate(preds):\n",
    "            shape = orig_img[i].shape if self.webcam else orig_img.shape\n",
    "            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def write_results(self, idx, preds, batch):\n",
    "        p, im, im0 = batch\n",
    "        all_outputs = []\n",
    "        log_string = \"\"\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "        self.seen += 1\n",
    "        im0 = im0.copy()\n",
    "        if self.webcam:  # batch_size >= 1\n",
    "            log_string += f'{idx}: '\n",
    "            frame = self.dataset.count\n",
    "        else:\n",
    "            frame = getattr(self.dataset, 'frame', 0)\n",
    "\n",
    "        self.data_path = p\n",
    "        save_path = str(self.save_dir / p.name)  # im.jpg\n",
    "        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')\n",
    "        log_string += '%gx%g ' % im.shape[2:]  # print string\n",
    "        self.annotator = self.get_annotator(im0)\n",
    "\n",
    "        det = preds[idx]\n",
    "        all_outputs.append(det)\n",
    "        if len(det) == 0:\n",
    "            return log_string\n",
    "        for c in det[:, 5].unique():\n",
    "            n = (det[:, 5] == c).sum()  # detections per class\n",
    "            log_string += f\"{n} {self.model.names[int(c)]}{'s' * (n > 1)}, \"\n",
    "        # write\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        xywh_bboxs = []\n",
    "        confs = []\n",
    "        oids = []\n",
    "        outputs = []\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            x_c, y_c, bbox_w, bbox_h = xyxy_to_xywh(*xyxy)\n",
    "            xywh_obj = [x_c, y_c, bbox_w, bbox_h]\n",
    "            xywh_bboxs.append(xywh_obj)\n",
    "            confs.append([conf.item()])\n",
    "            oids.append(int(cls))\n",
    "        xywhs = torch.Tensor(xywh_bboxs)\n",
    "        confss = torch.Tensor(confs)\n",
    "          \n",
    "        outputs = deepsort.update(xywhs, confss, oids, im0)\n",
    "        if len(outputs) > 0:\n",
    "            bbox_xyxy = outputs[:, :4]\n",
    "            identities = outputs[:, -2]\n",
    "            object_id = outputs[:, -1]\n",
    "            \n",
    "            draw_boxes(im0, bbox_xyxy, self.model.names, object_id,identities)\n",
    "\n",
    "        return log_string\n",
    "\n",
    "\n",
    "@hydra.main(version_base=None, config_path=str(DEFAULT_CONFIG.parent), config_name=DEFAULT_CONFIG.name)\n",
    "def predict(cfg):\n",
    "    init_tracker()\n",
    "    cfg.model = cfg.model or \"yolov8n.pt\"\n",
    "    cfg.imgsz = check_imgsz(cfg.imgsz, min_dim=2)  # check image size\n",
    "    cfg.source = cfg.source if cfg.source is not None else ROOT / \"assets\"\n",
    "    predictor = DetectionPredictor(cfg)\n",
    "    predictor()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983a174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
